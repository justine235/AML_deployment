{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# Connexion à l'environnement \n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET OF THIS NOTEBOOK\n",
    "# PART 1 : HUGGING FACE MODEL FROM MODEL CATALOG (azure ML)\n",
    "#==> deepset-gbert-base\n",
    "\n",
    "\n",
    "# PART 2 : HUGGING FACE MODEL FROM WEBSITE\n",
    "#==> https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct  |  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1 Hugging Face from Model Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml://registries/HuggingFace/models/deepset-gbert-base/versions/5\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.entities import (\n",
    "ManagedOnlineEndpoint,\n",
    "ManagedOnlineDeployment,\n",
    "Model,\n",
    "Environment,\n",
    "CodeConfiguration,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "\n",
    "registry_name = \"HuggingFace\"\n",
    "model_name = \"deepset-gbert-base\"\n",
    "version = \"5\"\n",
    "model_local_path = f\"azureml://registries/{registry_name}/models/{model_name}/versions/{version}\"\n",
    "print(model_local_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint créé : my-endpoint-202501231312\n"
     ]
    }
   ],
   "source": [
    "# création d'un endpoint\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Nom unique pour l'endpoint (on peut suffixer avec la date/heure)\n",
    "endpoint_name = \"my-endpoint-\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"Endpoint pour démo - déploie un modèle depuis le registry\",\n",
    "    auth_mode=\"key\"  # ou \"aml_token\"\n",
    ")\n",
    "\n",
    "# Crée l'endpoint (ou le met à jour s'il existe déjà)\n",
    "endpoint = ml_client.begin_create_or_update(endpoint).result()\n",
    "print(f\"Endpoint créé : {endpoint.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint my-endpoint-202501231312 exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................Déploiement créé : demo-deployment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
     ]
    }
   ],
   "source": [
    "# Déploiement sur l'endpoint\n",
    "\n",
    "deployment_name = \"demo-deployment\"\n",
    "\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model_local_path,\n",
    "    # Vous pouvez référencer un environnement existant dans votre workspace\n",
    "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:1\",  \n",
    "    instance_type=\"Standard_DS3_v2\",\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "deployment = ml_client.begin_create_or_update(deployment).result()\n",
    "print(f\"Déploiement créé : {deployment.name}\")\n",
    "\n",
    "# (Optionnel) Basculez tout le trafic vers ce déploiement\n",
    "endpoint.traffic = {deployment_name: 100}\n",
    "endpoint = ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more examples, testing here : https://github.com/azure/azureml-examples/blob/main/sdk/python/foundation-models/huggingface/inference/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2 : HUGGING FACE MODEL FROM WEBSITE\n",
    "#==> https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct  | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca7440093794491b0ff010dd5cd1dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6811cc62c9c74a63a8e000b5076a5098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416f7fec02314cc188e8765cb2949840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e54567f503149d4b5be563055bed592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0ad0e7f4da4bdabefa96397301b559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086f34126cc546fc937a4cc145cbe159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a2a421cf3c43f89d875d5b70133369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE:   0%|          | 0.00/11.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54242a1d82647999688bf7d9d495246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce06824c0a0941a6b93444d754b29449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee217edbffb4467c8fcfd7d6d74e3984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad67ec116f074d3b888a0c944a9a5bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c281c1cabe374c368d94b814230e91d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8e9676a61b42bab6baedaf6dcad96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/56.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531c9f7e1aa04ebfb21882b6a46dab98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5944c70b69b0488382c53e8d58fa69d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c610ed227b54259b1a29e5a4538337f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd8c2179da24032b53d4e5b46f7a60f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/jcharley2/code/Users/Saint-gobain/Qwen2-VL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "LOCAL_MODEL_DIR = \"./Qwen2-VL\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    local_dir=LOCAL_MODEL_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
      "\n",
      "Example: azcopy copy '/mnt/batch/tasks/shared/LS_root/mounts/clusters/jcharley2/code/Users/Saint-gobain/Qwen2-VL' 'https://stfqm45izfylnxqprod.blob.core.windows.net/azureml-blobstore-24beeade-8ebc-43d6-ba31-3bdbe9a007dc/LocalUpload/7cf4eab683d9d4ae74a14bd2fe5d7221/Qwen2-VL' \n",
      "\n",
      "See https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n",
      "\u001b[32mUploading Qwen2-VL (16594.41 MBs): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16594405493/16594405493 [01:24<00:00, 197295280.42it/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model registered successfully!\n",
      "Name: Qwen2-VL, Version: 1\n",
      "Model ID: /subscriptions/bf701399-a63b-44aa-bbbf-b848add18b4d/resourceGroups/rg-mlopsdemo-0001prod/providers/Microsoft.MachineLearningServices/workspaces/mlw-mlopsdemo-0001prod/models/Qwen2-VL/versions/1\n"
     ]
    }
   ],
   "source": [
    "# registration in azure ml\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
    "\n",
    "model_entity = Model(\n",
    "    name=\"Qwen2-VL\",\n",
    "    path=LOCAL_MODEL_DIR,  # local folder with all the downloaded files\n",
    "    description=\"A Hugging Face model downloaded locally and registered in Azure ML\"\n",
    ")\n",
    "\n",
    "registered_model = ml_client.models.create_or_update(model_entity)\n",
    "print(f\"Model registered successfully!\")\n",
    "print(f\"Name: {registered_model.name}, Version: {registered_model.version}\")\n",
    "print(f\"Model ID: {registered_model.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForVision2Seq\n",
    "from accelerate import infer_auto_device_map\n",
    "\n",
    "def init():\n",
    "    \"\"\"\n",
    "    Initialize the model and tokenizer while avoiding memory overload.\n",
    "    \"\"\"\n",
    "    global model, tokenizer\n",
    "\n",
    "    # Get model path\n",
    "    base_model_path = os.getenv(\"AZUREML_MODEL_DIR\", \"/var/azureml-app/azureml-models/Qwen2-VL/1\")\n",
    "    model_path = os.path.join(base_model_path, \"Qwen2-VL\")\n",
    "\n",
    "    print(f\"🔍 Checking model directory: {model_path}\")\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        raise RuntimeError(f\"Model path {model_path} not found!\")\n",
    "\n",
    "    config_path = os.path.join(model_path, \"config.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        raise RuntimeError(f\"config.json not found in {model_path}!\")\n",
    "\n",
    "    print(f\"config.json found at {config_path}\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "    # Use `device_map=\"auto\"` to manage memory safely\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",  # Ensures it loads on available memory\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        offload_folder=\"/tmp/model_offload\",  # Offload large weights to disk\n",
    "        offload_state_dict=True,  # Helps avoid memory fragmentation\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "def run(raw_data):\n",
    "    \"\"\"\n",
    "    Process an inference request.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(raw_data)\n",
    "        prompt = data.get(\"prompt\", \"Hello, how can I assist you?\")\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        return {\"response\": response}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting environment.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile environment.yml\n",
    "name: qwen2vl_env\n",
    "channels:\n",
    "  - defaults\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - pip\n",
    "  - pip:\n",
    "    - torch>=2.0\n",
    "    - transformers>=4.26\n",
    "    - accelerate>=0.16\n",
    "    - safetensors\n",
    "    - sentencepiece\n",
    "    - azureml-inference-server-http\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint : qwen2vl-endpoint-202501292031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint qwen2vl-endpoint-202501292031 exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environnement enregistré : qwen2vl-env\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading code (0.03 MBs): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27757/27757 [00:00<00:00, 393643.67it/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................................................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Déploiement : qwen2vl\n",
      "Trafic routé à 100% vers : qwen2vl\n",
      "\n",
      "=== Réponse du modèle ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Environment,\n",
    "    CodeConfiguration\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# 1) Connexion au workspace \n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
    "\n",
    "# 2) Création d'un endpoint  unique\n",
    "endpoint_name = \"qwen2vl-endpoint-\" + datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"Endpoint pour Qwen2-VL-7B-Instruct\",\n",
    "    auth_mode=\"key\"\n",
    ")\n",
    "\n",
    "endpoint = ml_client.begin_create_or_update(endpoint).result()\n",
    "print(f\"Endpoint : {endpoint.name}\")\n",
    "\n",
    "# 3) Création d'un environnement\n",
    "env = Environment(\n",
    "    name=\"qwen2vl-env\",\n",
    "    description=\"Env pour Qwen2-VL-7B-Instruct\",\n",
    "    conda_file=\"environment.yml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\"\n",
    ")\n",
    "registered_env = ml_client.environments.create_or_update(env)\n",
    "print(\"Environnement enregistré :\", registered_env.name)\n",
    "\n",
    "# 4) Code + scoring config\n",
    "code_config = CodeConfiguration(\n",
    "    code=\".\",            \n",
    "    scoring_script=\"score.py\"\n",
    ")\n",
    "\n",
    "# 5) Déploiement (ManagedOnlineDeployment) depuis le modèle enregistré\n",
    "model_name = \"Qwen2-VL\" \n",
    "model_version = \"1\"  #  \"latest\" \n",
    "model_path_in_aml = f\"azureml:{model_name}:{model_version}\"\n",
    "\n",
    "deployment_name = \"qwen2vl\"\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=endpoint_name,\n",
    "    environment=env,\n",
    "    code_configuration=code_config,\n",
    "    instance_type=\"Standard_NC6s_v3\",\n",
    "    instance_count=1,\n",
    "    # Ici, on passe le modèle enregistré\n",
    "    model=model_path_in_aml  \n",
    ")\n",
    "\n",
    "deployment = ml_client.begin_create_or_update(deployment).result()\n",
    "print(f\"Déploiement : {deployment.name}\")\n",
    "\n",
    "# 6) Diriger le trafic à 100% vers le déploiement\n",
    "endpoint.traffic = {deployment_name: 100}\n",
    "endpoint = ml_client.begin_create_or_update(endpoint).result()\n",
    "print(f\"Trafic routé à 100% vers : {deployment.name}\")\n",
    "\n",
    "# 7) Test d'inférence\n",
    "endpoint_key = ml_client.online_endpoints.get_keys(endpoint_name).primary_key\n",
    "endpoint_uri = endpoint.scoring_uri\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {endpoint_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"prompt\": \"Présente-toi en français.\"\n",
    "}\n",
    "\n",
    "resp = requests.post(endpoint_uri, headers=headers, data=json.dumps(payload))\n",
    "print(\"\\n=== Réponse du modèle ===\")\n",
    "print(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option alternative : Déploiement en VLLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
